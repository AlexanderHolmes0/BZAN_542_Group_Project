---
title: "BZAN542_Group_Project"
author: "Alexander Holmes"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
library(embed)
library(stringdist)
library(probably)
library(bonsai)
library(textrecipes)
library(finetune)
#load("Trained_Models.RData")
```

```{r pkgs}
#pkgs <- 
#  c("bonsai", "doParallel", "embed", "finetune", "lightgbm", "lme4",
#    "plumber", "probably", "ranger", "rpart", "rpart.plot", "rules",
#    "splines2", "stacks", "text2vec", "textrecipes", "tidymodels", 
#    "vetiver", "remotes","textrecipes")
#
#install.packages(pkgs)
```

```{r Parallel Processing}
cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)
```



```{r split}
set.seed(123)
car_sales <- read.csv('pakwheels_used_car_data_v02.csv',stringsAsFactors = FALSE,na.strings = c("NA",""))
car_sales <- car_sales |> 
  mutate(luxury = factor(ifelse(make %in%
  c("Land", "Porsche","Bentley","Jaguar","Mercedes","Range","Tesla")
  , 1, 0)),
  usd = price*0.003541,
  assembly=replace_na(assembly,"Local"))
car_sales <- car_sales |> 
  filter(addref!=7770572)

car_sales <- car_sales[complete.cases(car_sales),]

kbb_colors_df <- data.frame(
  color = c("Black", "White", "Silver", "Gray", "Blue", "Red", "Green", "Brown", "Yellow", "Orange","Beige","Maroon","Gold","Bronze","Burgandy","Magenta","Pink","Purple","Navy","Graphite Grey","")
)

car_sales <- car_sales |> 
  mutate(fuzzy_colors = kbb_colors_df[amatch(car_sales$color, kbb_colors_df$color, maxDist = 10),]) |> 
  mutate(across(where(is.character),as.factor))


car_split <- initial_split(car_sales,strata = price)
car_train <- training(car_split)
car_test <- testing(car_split)
car_folds <- vfold_cv(car_train, v = 5, strata = usd)


ggplot(car_train, aes(x = year)) +
  geom_histogram(binwidth = 1) +
  labs(x = "Year", y = "Count", title = "Distribution of Car Year") +
  facet_wrap(~ body) +
  theme_minimal()
ggplot(car_train, aes(x = log(usd))) +
  geom_histogram() +
  labs(x = "Price (USD)", y = "Count", title = "Distribution of Car Price") +
  theme_minimal()
ggplot(car_train, aes(x = mileage)) +
  geom_histogram(binwidth = 10000) +
  labs(x = "Mileage (Km)", y = "Count", title = "Distribution of Car Mileage") +
  theme_minimal()
#stacked bar chart
ggplot(car_train, aes(x = fuel, fill = fuel)) +
  geom_bar() +
  labs(x = "Fuel Type", y = "Count", title = "Distribution of Car Fuel Type") +
  facet_wrap(~ transmission) +
  theme_minimal()

#stacked bar chart
ggplot(car_sales, aes(x = assembly, fill = assembly)) +
  geom_bar() +
  labs(x = "Assembly", y = "Count", title = "Distribution of Car Assembly") +
  facet_wrap(~ transmission) +
  theme_minimal()
#rotate x axis labels
ggplot(car_sales,aes(x=body,y=log10(usd),fill=body))+
  geom_boxplot()+
  coord_flip()+
  theme(legend.position = "none")

ggplot(car_sales |> group_by(make) |> summarise(count=n(),average_usd=mean(usd)) |> filter(count>10) |> mutate(make=fct_reorder(make,average_usd)),aes(x=make,y=average_usd,fill=make))+
  geom_bar(stat='identity')+
  coord_flip()+
  theme(legend.position = "none")+
  scale_y_continuous(labels=scales::dollar)

ggplot(car_sales, aes(x=as.factor(fuzzy_colors),y=log(usd)))+
  geom_boxplot()+coord_flip()+theme(legend.position = "none")+labs(x="Color",y="Price (USD)")

```

```{r recipe}
car_recipe <- recipe(usd ~ .,data=car_train) |> 
  step_rm(addref,color,price) |> 
  step_lencode_mixed(all_nominal_predictors(),-fuel,-transmission,-assembly ,outcome = vars(usd)) |> 
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_other(all_nominal_predictors(),threshold = .3) |> 
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9)
juice(prep(car_recipe))
```


```{r workflow}
car_workflow <- workflow() |> 
  add_recipe(car_recipe) |> 
  add_model(linear_reg())

reg_metrics <- metric_set(rmse,mae, rsq)

car_results <- car_workflow |>
  fit_resamples(resamples = car_folds,control = control_resamples(save_pred = T),metrics = reg_metrics)
collect_metrics(car_results)
```

```{r workflow_results}
cal_plot_regression(car_results, alpha = 1 / 5)
#talk about leverage!
```
```{r}

lgbm_spec <- 
  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("lightgbm")

lgbm_wflow <- workflow(car_recipe, lgbm_spec)

set.seed(12)
grid <- 
  lgbm_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

ctrl <- control_grid(save_pred = TRUE,verbose=TRUE)

lgbm_res <-
  lgbm_wflow %>%
  tune_grid(
    resamples = car_folds,
    control = ctrl,
    metrics = reg_metrics,
    grid=grid
  )

collect_metrics(lgbm_res)
show_best(lgbm_res, metric = "rmse")
lgbm_best <- select_best(lgbm_res, metric = "rmse")
#select_best(lgbm_res, metric = "rmse")
```


```{r}

lgbm_res %>%
  collect_predictions(
    parameters = lgbm_best
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )
```

```{r Racing}
set.seed(123)
lgbm_race_res <-
  lgbm_wflow %>%
  tune_race_anova(
    resamples = car_folds,
    grid = 50, 
    metrics = reg_metrics,
    control=control_race(save_pred = TRUE,verbose=TRUE)
  )
show_best(lgbm_race_res,metric='rmse')

plot_race(lgbm_race_res) + 
  scale_x_continuous(breaks = pretty_breaks())

lgbm_race_res %>%
  collect_predictions(
    parameters = select_best(lgbm_race_res,metric='rmse')
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )

```

```{r lock down best}
best_param <- select_best(lgbm_race_res,metric='rmse')

final_wflow <- 
  lgbm_wflow %>%
  finalize_workflow(best_param)

```

```{r final model}
set.seed(123)
final_res <- 
  final_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_res |> collect_metrics()
```


#lets see if XGBoost can do anybetter/?

```{r}
xgb_spec <- boost_tree( #model spec basically showing what we wanna do    
    trees = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    sample_size = tune(),
    tree_depth = tune(),
    loss_reduction = tune()) %>% 
  set_engine("xgboost") %>% #see ?set_engine for a full list of possibilites
  set_mode("regression")

xgb_wf <- workflow() %>%  #add the preproc with the model spec
  add_recipe(car_recipe) %>% 
  add_model(xgb_spec)

xgb_grid <- grid_latin_hypercube( 
  #cover all bases in the ~7 dimensional space of possible hyper params
  trees(range = c(300,2400)),
  tree_depth(range = c(4,20)),
  min_n(range = c(1,10)),
  loss_reduction(),
  sample_size = sample_prop(range = c(.4,.9)),
  mtry(range = c(4,12)),
  learn_rate(range = c(-4,-1)),
  size = 10
  )

xgb_rs <- tune_race_anova(
  object = xgb_wf,
  resamples = car_folds,
  metrics = metric_set(rmse),
  grid = xgb_grid, #number of each different hyperparams to test out
  control = control_race(verbose_elim = TRUE)
)

plot_race(xgb_rs) + 
  scale_x_continuous(breaks = pretty_breaks())
show_best(xgb_rs,metric='rmse')
```

















# Workflow Sets

```{r}
base_recipe <- 
   recipe(usd ~ ., data = car_train) |> 
  step_rm(addref,color,price) |> 
  step_lencode_mixed(all_nominal_predictors(), outcome = vars(usd)) |> 
  step_normalize(all_predictors()) 

filter_rec <- 
   base_recipe %>% 
   step_corr(all_numeric_predictors(), threshold = tune())

pca_rec <- base_recipe %>% 
   step_pca(all_numeric_predictors(), num_comp = tune()) |> 
  step_normalize(all_predictors())
```

```{r}
library(rules)
library(baguette)

regularized_spec <- 
   linear_reg(penalty = tune(), mixture = tune()) %>% 
   set_engine("glmnet")

cart_spec <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

nnet_spec <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")
```


```{r}
wf_set <- 
   workflow_set(
      preproc = list(car_recipe),
      models = list(glmnet = regularized_spec, cart = cart_spec, 
                    RF = rf_spec, xgboost = xgb_spec),
      cross = TRUE
   )

wf_set <- 
   wf_set %>% 
   anti_join(tibble(wflow_id = c("pca_glmnet", "filter_glmnet")), 
             by = "wflow_id")
```



```{r fit the set}
grid_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)

wf_set_res <- 
   wf_set %>% 
   workflow_map("tune_grid", resamples = car_folds, grid = 5, 
                metrics = reg_metrics, control = grid_ctrl,verbose = TRUE,seed=123) 
autoplot(wf_set_res, metric = "rmse")

library(stacks)

wf_set_stack <- 
  stacks() %>% 
  add_candidates(wf_set_res)

set.seed(122)
wf_set_stack_res <- blend_predictions(wf_set_stack)
autoplot(wf_set_stack_res)
autoplot(wf_set_stack_res, type = "weights")
wf_set_stack_res <- fit_members(wf_set_stack_res)

autoplot(wf_set_stack_res,type="performance")

predict(wf_set_stack_res, car_test) %>% 
  bind_cols(car_test) %>% 
  cal_plot_regression(truth = usd, estimate = .pred, alpha = 1 / 4)

```

```{r set racing}
race_results <-
   all_workflows %>%
   workflow_map(
      "tune_race_anova",
      seed = 1503,
      resamples = car_folds,
      grid = 25,
      control = grid_ctrl
   )
```


---
title: "BZAN542_Group_Project"
author: "Alexander Holmes"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = T)
library(tidymodels)
library(tidyverse)
library(embed)
library(stringdist)
library(probably)
library(bonsai)
library(textrecipes)
library(finetune)
library(plotly)
theme_set(theme_bw())
#load("Stacked_Models.RData")
#load('Trained_Models.RData')
load('Filtered_Out_Luxury_Models.RData')
```

```{r pkgs}
#pkgs <- 
#  c("bonsai", "doParallel", "embed", "finetune", "lightgbm", "lme4",
#    "plumber", "probably", "ranger", "rpart", "rpart.plot", "rules",
#    "splines2", "stacks", "text2vec", "textrecipes", "tidymodels", 
#    "vetiver", "remotes","textrecipes","agua")
#
#install.packages(pkgs)
```

```{r Parallel Processing}
cores <- parallelly::availableCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(cores)
doParallel::registerDoParallel(cl)
```

```{r split}
set.seed(123)
car_sales <- read.csv('pakwheels_used_car_data_v02.csv',stringsAsFactors = FALSE,na.strings = c("NA",""))
car_sales <- car_sales |> 
  mutate(luxury = factor(ifelse(make %in%
  c("Land", "Porsche","Bentley","Jaguar","Mercedes","Range","Tesla",'Audi','Austin','BMW',"Lexus")
  , 1, 0)),
  engine=as.factor(engine),
  registered=ifelse(registered=="Un-Registered",0,1),
  usd = price*0.003541,
  #miles = mileage*0.621371,
  assembly=replace_na(assembly,"Local")) |> 
  filter(addref!=7770572 & luxury == 0) |> 
  select(-city,-addref) |> 
  filter(!(make =='Ford' & body == "Double Cabin"))

car_sales |> 
  filter(make=='Ford')

car_sales <- car_sales[complete.cases(car_sales),]

kbb_colors_df <- data.frame(
  color = c("Black", "White", "Silver", "Gray", "Blue", "Red", "Green", "Brown", "Yellow", "Orange","Beige","Maroon","Gold","Bronze","Burgandy","Magenta","Pink","Purple","Navy","Graphite Grey","")
)

car_sales <- car_sales |> 
  mutate(fuzzy_colors = kbb_colors_df[amatch(car_sales$color, kbb_colors_df$color,method = 'jaccard',maxDist = 11),])


car_split <- initial_split(car_sales,strata = price)
car_train <- training(car_split)
car_test <- testing(car_split)
car_folds <- vfold_cv(car_train, v = 5, strata = usd)


ggplot(car_train, aes(x = year)) +
  geom_histogram(binwidth = 1) +
  labs(x = "Year", y = "Count", title = "Distribution of Car Year") +
  facet_wrap(~ body) +
  theme_minimal()
ggplot(car_train, aes(x = log(usd))) +
  geom_histogram() +
  labs(x = "Price (USD)", y = "Count", title = "Distribution of Car Price") +
  theme_minimal()
ggplot(car_train, aes(x = mileage)) +
  geom_histogram(binwidth = 10000) +
  labs(x = "Mileage (Km)", y = "Count", title = "Distribution of Car Mileage") +
  theme_minimal()
#stacked bar chart
ggplot(car_train, aes(x = fuel, fill = fuel)) +
  geom_bar() +
  labs(x = "Fuel Type", y = "Count", title = "Distribution of Car Fuel Type") +
  facet_wrap(~ transmission) +
  theme_minimal()

#stacked bar chart
ggplot(car_sales, aes(x = assembly, fill = assembly)) +
  geom_bar() +
  labs(x = "Assembly", y = "Count", title = "Distribution of Car Assembly") +
  facet_wrap(~ transmission) +
  theme_minimal()
#rotate x axis labels
ggplot(car_sales,aes(x=body,y=log10(usd),fill=body))+
  geom_boxplot()+
  coord_flip()+
  theme(legend.position = "none")

ggplot(car_sales |> group_by(make) |> summarise(count=n(),average_usd=mean(usd)) |> filter(count>10) |> mutate(make=fct_reorder(make,average_usd)),aes(x=make,y=average_usd,fill=make))+
  geom_bar(stat='identity')+
  coord_flip()+
  theme(legend.position = "none")+
  scale_y_continuous(labels=scales::dollar)

ggplot(car_sales, aes(x=as.factor(fuzzy_colors),y=log(usd)))+
  geom_boxplot()+coord_flip()+theme(legend.position = "none")+labs(x="Color",y="Price (USD)")
#more exploratory data analysis plots not by make; there are other columns like mileage and price

#scatterplot
ggplot(car_train, aes(x = mileage, y = usd)) +
  geom_point() +
  geom_smooth()+
  labs(x = "Mileage (Km)", y = "Price (USD)", title = "Price vs Mileage") +
  theme_minimal()
#scatterplot
ggplot(car_train, aes(x = mileage, y = log(usd))) +
  geom_point(alpha=.1) +
  geom_smooth()+
  labs(x = "Mileage (KM)", y = "Log (USD)", title = "Price vs Year") +
  theme_minimal()
#scatterplot
ggplot(car_train, aes(x = log(mileage), y = log(usd))) +
  geom_point() +
  geom_smooth()+
  labs(x = "Miles", y = "Price (USD)", title = "Price vs Year") +
  theme_minimal()

```

```{r recipe}
car_recipe <- recipe(usd ~ .,data=car_train) |> 
  step_rm(price,luxury) |> 
  step_lencode_mixed(all_nominal_predictors(),-fuel,-transmission,
                     -assembly,outcome = vars(usd)) |> 
  step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_other(all_nominal_predictors(),threshold = .3) |> 
  step_zv(all_predictors()) |>
  step_normalize(all_numeric_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9)

juice(prep(car_recipe))

bake(prep(car_recipe),new_data=car_test)
ggplot(juice(prep(car_recipe)) |> select(usd),aes(x=usd))+
  geom_histogram()
```

```{r workflow}
glmnet_spec <- linear_reg(penalty = tune(),
                        mixture = tune()) %>%
  set_engine("glmnet")

car_workflow <- workflow() |> 
  add_recipe(car_recipe) |> 
  add_model(glmnet_spec)

reg_metrics <- metric_set(rmse,mae, rsq)

car_results <- car_workflow |>
  tune_grid(resamples = car_folds,
                grid = 25,
                control = control_resamples(save_pred = T),
                metrics = reg_metrics)

collect_metrics(car_results)

kableExtra::kbl(show_best(car_results,metric='rmse',n=1) |> 
bind_rows(show_best(car_results,metric="rsq",n=1)) |> 
  select(.metric,mean,std_err),caption = "GLMNET Training Results",booktabs = T) |> 
  kableExtra::kable_styling(latex_options = c("striped", "hold_position"),full_width = F)

glmnet_best <- car_results %>% select_by_one_std_err(mixture, metric = "rmse")
collect_metrics(glmnet_best)
```

```{r workflow_results}
car_results %>%
  collect_predictions(
    parameters = glmnet_best
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  ) +
  labs(title = "GLMNET Model Predictions")
#talk about leverage!
```

```{r}

final_linear <- 
  car_workflow %>%
  finalize_workflow(glmnet_best)

final_linear_res <- 
  final_linear %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_linear_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_linear_res |> collect_metrics() |> filter(.metric %in% c('rmse','rsq'))
linear_pred <- final_linear_res |> collect_predictions()
linear_pred |> 
  mutate(resid=usd-.pred) |> 
  arrange(desc(abs(resid)))

```

# Light GBM

```{r}

lgbm_spec <- 
  boost_tree(trees = tune(), learn_rate = tune(), min_n = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("lightgbm")

lgbm_wflow <- workflow(car_recipe, lgbm_spec)

set.seed(12)
grid <- 
  lgbm_wflow %>% 
  extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 20)

#visualize the grid in 3d space with plotly and tooltip
plot_ly(grid, x = ~trees, y = ~log10(learn_rate), z = ~min_n) %>%
  add_markers(color = ~trees) %>%
  layout(scene = list(xaxis = list(title = "Trees"),
                      yaxis = list(title = "Learn Rate"),
                      zaxis = list(title = "Min N")))

ctrl <- control_grid(save_pred = TRUE,verbose=TRUE)

lgbm_res <-
  lgbm_wflow %>%
  tune_grid(
    resamples = car_folds,
    control = ctrl,
    metrics = reg_metrics,
    grid=grid
  )

collect_metrics(lgbm_res)
show_best(lgbm_res, metric = c("rmse"))
show_best(lgbm_res, metric = c("rsq"))
lgbm_best <- select_best(lgbm_res, metric = "rmse")
select_by_one_std_err(lgbm_res, metric = "rmse",desc(trees))
#select_best(lgbm_res, metric = "rmse")

```

```{r}
lgbm_res %>%
  collect_predictions(
    parameters = lgbm_best
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )
autoplot(lgbm_res,metric = "rmse")+geom_smooth()
```

```{r Racing}
set.seed(123)
lgbm_race_res <-
  lgbm_wflow %>%
  tune_race_anova(
    resamples = car_folds,
    grid = grid, 
    metrics = reg_metrics,
    control=control_race(save_pred = TRUE,verbose=TRUE)
  )

show_best(lgbm_race_res,metric='rmse')

plot_race(lgbm_race_res) + 
  scale_x_continuous(breaks = pretty_breaks())

lgbm_race_res %>%
  collect_predictions(
    parameters = select_best(lgbm_race_res,metric='rmse')
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )

```

```{r lock down best}
best_param <- select_best(lgbm_race_res,metric='rmse')

final_wflow <- 
  lgbm_wflow %>%
  finalize_workflow(best_param)

```

```{r final model}
set.seed(123)
final_res <- 
  final_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_res |> collect_metrics()
```

#lets see if XGBoost can do anybetter/?

```{r}
xgb_spec <- boost_tree( #model spec basically showing what we wanna do    
    trees = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = tune(),
    sample_size = tune(),
    tree_depth = tune(),
    loss_reduction = tune()) %>% 
  set_engine("xgboost") %>% #see ?set_engine for a full list of possibilites
  set_mode("regression")

xgb_wf <- workflow() %>%  #add the preproc with the model spec
  add_recipe(car_recipe) %>% 
  add_model(xgb_spec)

xgb_grid <- grid_latin_hypercube( 
  #cover all bases in the ~7 dimensional space of possible hyper params
  trees(range = c(300,1000)),
  tree_depth(range = c(4,20)),
  min_n(range = c(1,10)),
  loss_reduction(),
  sample_size = sample_prop(range = c(.4,.9)),
  mtry(range = c(4,12)),
  learn_rate(range = c(-4,-1)),
  size = 10
  )

xgb_rs <- tune_race_anova(
  object = xgb_wf,
  resamples = car_folds,
  metrics = reg_metrics,
  grid = xgb_grid, #number of each different hyperparams to test out
  control = control_race(save_pred = TRUE,verbose=TRUE)
)

plot_race(xgb_rs) + 
  scale_x_continuous(breaks = pretty_breaks())

show_best(xgb_rs,metric='rmse')


best_xgb <- select_by_one_std_err(xgb_rs, metric = "rmse",desc(trees))

final_xgb_wflow <- 
  xgb_wf %>%
  finalize_workflow(best_xgb)

set.seed(123)
final_res_xgb <- 
  final_xgb_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_res_xgb %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_res_xgb |> collect_metrics()
```

# Knn 
```{r}
KNNSpec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%  
  set_engine("kknn") %>% 
  set_mode("regression")

KNN_workflow <- workflows::workflow() %>%
  workflows::add_recipe(car_recipe) %>%
  workflows::add_model(KNNSpec)

KNN_Work_Spec <-
  KNN_workflow %>%
  tune_race_anova(
    resamples = car_folds,
    grid = 5, 
    metrics = reg_metrics,
    control=control_race(save_pred = TRUE,verbose=TRUE)
  )
show_best(KNN_Work_Spec,metric='rmse')
select_by_one_std_err(KNN_Work_Spec, metric = "rmse",desc(neighbors))

plot_race(KNN_Work_Spec) + 
  scale_x_continuous(breaks = pretty_breaks())

autoplot(KNN_Work_Spec,metric = "rmse")+geom_smooth()

KNN_Work_Spec %>%
  collect_predictions(
    parameters = select_best(KNN_Work_Spec,metric='rmse')
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )

finalKNN_wflow <- 
  KNN_workflow %>%
  finalize_workflow(select_by_one_std_err(KNN_Work_Spec, metric = "rmse",desc(neighbors)))

set.seed(123)
final_KNN_res <- 
  finalKNN_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_KNN_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_KNN_res |> collect_metrics()
```

# DECISION TREE #Cart
```{r}

DT_Spec <- decision_tree(tree_depth = tune(), 
                          min_n = tune(), 
                          cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

DT_workflow <- workflows::workflow() %>%
  workflows::add_recipe(car_recipe) %>%
  workflows::add_model(DT_Spec)

DT_Train <-
  DT_workflow %>%
  tune_race_anova(
    resamples = car_folds,
    grid = 25, 
    metrics = reg_metrics,
    control=control_race(save_pred = TRUE,verbose=TRUE)
  )

show_best(DT_Train,metric='rmse')
select_by_one_std_err(DT_Train, metric = "rmse",desc(tree_depth))

plot_race(DT_Train) + 
  scale_x_continuous(breaks = pretty_breaks())

autoplot(DT_Train,metric = "rmse")+geom_smooth()

DT_Train %>%
  collect_predictions(
    parameters = select_best(DT_Train,metric='rmse')
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )

finalDT_wflow <- 
  DT_workflow %>%
  finalize_workflow(select_by_one_std_err(DT_Train, metric = "rmse",desc(tree_depth)))

set.seed(123)
final_DT_res <- 
  finalDT_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

final_DT_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

final_DT_res |> collect_metrics()
```

# Brulee NNET

```{r}
# Use mlp() for multi-layer perceptron
nn_spec <- mlp(
  hidden_units = tune(),
  penalty = tune(),
  epochs = tune(),
  learn_rate = tune(),
  activation = 'relu'
) %>%  
  set_engine("brulee") %>% 
  set_mode("regression")

NNET_workflow <- workflows::workflow() %>%
  workflows::add_recipe(car_recipe) %>%
  workflows::add_model(nn_spec)

set.seed(123)
NET_Race_res <-
  NNET_workflow %>%
  tune_race_anova(
    resamples = car_folds,
    grid = 20, 
    metrics = reg_metrics,
    control=control_race(save_pred = TRUE,verbose=TRUE)
  )

show_best(NET_Race_res,metric='rmse')

plot_race(NET_Race_res) + 
  scale_x_continuous(breaks = pretty_breaks())

NET_Race_res %>%
  collect_predictions(
    parameters = select_best(NET_Race_res,metric='rmse')
  ) %>%
  cal_plot_regression(
    truth = usd,
    estimate = .pred,
    alpha = 1 / 3
  )

finalNNET_wflow <- 
  NNET_workflow %>%
  finalize_workflow(select_by_one_std_err(NET_Race_res, metric = "rmse",desc(hidden_units)))

set.seed(123)
NET_Final_res <- 
  finalNNET_wflow %>% 
  last_fit(
    split = car_split,
    metrics = reg_metrics
  )

NET_Final_res %>% 
  collect_predictions() %>% 
  cal_plot_regression(
    truth = usd, 
    estimate = .pred, 
    alpha = 1 / 4)

NET_Final_res |> collect_metrics()
```





# DALEXtra

```{r}
load("ModelsWithExplainer.RData")
library(DALEXtra)
final_model <- extract_workflow(final_res)

model_explainer <- explain_tidymodels(final_model, data = select(car_train,-usd), y = car_train$usd)
model_parts <- model_parts(model_explainer)
plot(model_parts,max_vars= 10)

resids_gbm <- model_performance(model_explainer)
plot(resids_gbm)
model_profile <- model_profile(model_explainer, type = "accumulated")
plot(model_profile, variables = "mileage")
example_car <- car_train[sample(1:nrow(car_train),1), ]

prediction_breakdown <- predict_parts(
  explainer = model_explainer,
  new_observation = example_car,
  type = "break_down"
)

example_car
plot(prediction_breakdown)
```

# Model Diagnostics

```{r}
final_res %>% 
  collect_predictions() %>% 
  ggplot(aes(x = 1/sqrt(usd) , y = usd-.pred)) +
  geom_point(alpha = 1 / 4) +
  geom_smooth()+
  labs(x = "Predicted", y = "Observed")

ggplot(q1, aes(sample = model$resid)) +
  stat_qq() +
  stat_qq_line() +
  theme_bw() +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
    title = "Figure 3. Normal Q-Q Plot"
  )
```

# Workflow Sets

```{r}
base_recipe <- 
   recipe(usd ~ ., data = car_train) |> 
  step_rm(addref,color,price) |> 
  step_lencode_mixed(all_nominal_predictors(), outcome = vars(usd)) |> 
  step_normalize(all_predictors()) 

filter_rec <- 
   base_recipe %>% 
   step_corr(all_numeric_predictors(), threshold = tune())

pca_rec <- base_recipe %>% 
   step_pca(all_numeric_predictors(), num_comp = tune()) |> 
  step_normalize(all_predictors())
```

```{r}
library(rules)
library(baguette)

regularized_spec <- 
   linear_reg(penalty = tune(), mixture = tune()) %>% 
   set_engine("glmnet")

cart_spec <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

nnet_spec <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")
```

```{r}
wf_set <- 
   workflow_set(
      preproc = list(car_recipe),
      models = list(glmnet = regularized_spec, cart = cart_spec, 
                    RF = rf_spec, xgboost = xgb_spec),
      cross = TRUE
   )

wf_set <- 
   wf_set %>% 
   anti_join(tibble(wflow_id = c("pca_glmnet", "filter_glmnet")), 
             by = "wflow_id")
```

```{r fit the set}
grid_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)

wf_set_res <- 
   wf_set %>% 
   workflow_map("tune_grid", resamples = car_folds, grid = 5, 
                metrics = reg_metrics, control = grid_ctrl,verbose = TRUE,seed=123) 
autoplot(wf_set_res, metric = "rmse")

library(stacks)

wf_set_stack <- 
  stacks() %>% 
  add_candidates(wf_set_res)

set.seed(122)
wf_set_stack_res <- blend_predictions(wf_set_stack)
autoplot(wf_set_stack_res)
autoplot(wf_set_stack_res, type = "weights")
wf_set_stack_res <- fit_members(wf_set_stack_res)

autoplot(wf_set_stack_res,type="performance")

predict(wf_set_stack_res, car_test) %>% 
  bind_cols(car_test) %>% 
  cal_plot_regression(truth = usd, estimate = .pred, alpha = 1 / 4)

stack_pred <- predict(wf_set_stack_res, car_test) |> 
  bind_cols(car_test)

stack_pred |> 
  rmse(truth = usd, estimate = .pred)

```

```{r set racing}
race_results <-
   all_workflows %>%
   workflow_map(
      "tune_race_anova",
      seed = 1503,
      resamples = car_folds,
      grid = 25,
      control = grid_ctrl
   )
```
